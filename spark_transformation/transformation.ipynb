{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3307b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9eba8c-5067-4696-bc95-f00617c5adc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:45:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/03/18 12:45:58 INFO SharedState: Warehouse path is 'file:/home/chenchen/DE-Zoomcamp-Project/spark_transformation/spark-warehouse'.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data_Transformation\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.jars\", \"gcs-connector-hadoop3-latest.jar,spark-bigquery-latest_2.12.jar\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/home/chenchen/.gc/my-creds.json\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa6a2bc-3311-4903-97b5-b4ef134b5509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:46:11 INFO GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=749; previousMaxLatencyMs=0; operationCount=1; context=gs://de-zoomcamp-project-453801-terra-bucket/stock_dataset/stock/1742135256.3842237.3b60b69d81.csv.gz\n",
      "25/03/18 12:46:11 INFO InMemoryFileIndex: It took 88 ms to list leaf files for 1 paths.\n",
      "25/03/18 12:46:11 INFO InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.\n",
      "25/03/18 12:46:14 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/03/18 12:46:14 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/03/18 12:46:14 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "25/03/18 12:46:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 201.5 KiB, free 434.2 MiB)\n",
      "25/03/18 12:46:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)\n",
      "25/03/18 12:46:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 34.6 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:46:15 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "25/03/18 12:46:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/03/18 12:46:16 INFO CodeGenerator: Code generated in 244.004158 ms\n",
      "25/03/18 12:46:16 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "25/03/18 12:46:16 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/03/18 12:46:16 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "25/03/18 12:46:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/03/18 12:46:16 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 12:46:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/03/18 12:46:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 17.8 KiB, free 434.2 MiB)\n",
      "25/03/18 12:46:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.1 MiB)\n",
      "25/03/18 12:46:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 8.0 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:46:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 12:46:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 12:46:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/03/18 12:46:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, PROCESS_LOCAL, 4972 bytes) taskResourceAssignments Map()\n",
      "25/03/18 12:46:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/03/18 12:46:16 INFO FileScanRDD: Reading File path: gs://de-zoomcamp-project-453801-terra-bucket/stock_dataset/stock/1742135256.3842237.3b60b69d81.csv.gz, range: 0-23526, partition values: [empty row]\n",
      "25/03/18 12:46:16 INFO CodeGenerator: Code generated in 19.32386 ms\n",
      "25/03/18 12:46:17 INFO CodecPool: Got brand-new decompressor [.gz]\n",
      "25/03/18 12:46:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1805 bytes result sent to driver\n",
      "25/03/18 12:46:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 578 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 12:46:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/03/18 12:46:17 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.792 s\n",
      "25/03/18 12:46:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/03/18 12:46:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/03/18 12:46:17 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.848648 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:46:17 INFO CodeGenerator: Code generated in 14.231533 ms\n",
      "25/03/18 12:46:17 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "25/03/18 12:46:17 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/03/18 12:46:17 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "25/03/18 12:46:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/03/18 12:46:17 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 12:46:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/03/18 12:46:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 18.4 KiB, free 434.1 MiB)\n",
      "25/03/18 12:46:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 434.1 MiB)\n",
      "25/03/18 12:46:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 8.8 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:46:17 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 12:46:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 12:46:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "25/03/18 12:46:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, PROCESS_LOCAL, 4972 bytes) taskResourceAssignments Map()\n",
      "25/03/18 12:46:17 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "25/03/18 12:46:17 INFO FileScanRDD: Reading File path: gs://de-zoomcamp-project-453801-terra-bucket/stock_dataset/stock/1742135256.3842237.3b60b69d81.csv.gz, range: 0-23526, partition values: [empty row]\n",
      "25/03/18 12:46:17 INFO CodecPool: Got brand-new decompressor [.gz]\n",
      "25/03/18 12:46:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1556 bytes result sent to driver\n",
      "25/03/18 12:46:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 312 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 12:46:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/03/18 12:46:17 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0.334 s\n",
      "25/03/18 12:46:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/03/18 12:46:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/03/18 12:46:17 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0.346883 s\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .option(\"encoding\", \"us-ascii\") \\\n",
    "    .csv(\"gs://de-zoomcamp-project-453801-terra-bucket/stock_dataset/stock/1742135256.3842237.3b60b69d81.csv.gz\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104b40ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:46:17 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/03/18 12:46:17 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/03/18 12:46:17 INFO FileSourceStrategy: Output Data Schema: struct<open: double, high: double, low: double, close: double, volume: double ... 15 more fields>\n",
      "25/03/18 12:46:18 INFO CodeGenerator: Code generated in 51.916017 ms\n",
      "25/03/18 12:46:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 201.4 KiB, free 433.9 MiB)\n",
      "25/03/18 12:46:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 433.9 MiB)\n",
      "25/03/18 12:46:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 34.6 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:46:18 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0\n",
      "25/03/18 12:46:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/03/18 12:46:18 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/03/18 12:46:18 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/03/18 12:46:18 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/03/18 12:46:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/03/18 12:46:18 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 12:46:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/03/18 12:46:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 23.0 KiB, free 433.9 MiB)\n",
      "25/03/18 12:46:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.3 KiB, free 433.9 MiB)\n",
      "25/03/18 12:46:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 9.3 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:46:18 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 12:46:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 12:46:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/03/18 12:46:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, PROCESS_LOCAL, 4972 bytes) taskResourceAssignments Map()\n",
      "25/03/18 12:46:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "25/03/18 12:46:18 INFO FileScanRDD: Reading File path: gs://de-zoomcamp-project-453801-terra-bucket/stock_dataset/stock/1742135256.3842237.3b60b69d81.csv.gz, range: 0-23526, partition values: [empty row]\n",
      "25/03/18 12:46:18 INFO CodeGenerator: Code generated in 46.20669 ms\n",
      "25/03/18 12:46:18 INFO CodecPool: Got brand-new decompressor [.gz]\n",
      "25/03/18 12:46:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2224 bytes result sent to driver\n",
      "25/03/18 12:46:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 263 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 12:46:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/03/18 12:46:18 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.285 s\n",
      "25/03/18 12:46:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/03/18 12:46:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/03/18 12:46:18 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.292127 s\n",
      "25/03/18 12:46:18 INFO CodeGenerator: Code generated in 47.513364 ms\n",
      "+-------+--------+------+------+-----------+--------+--------+---------+--------+-----------+------------+--------+------+--------+-------------------+--------------------+--------------+\n",
      "|   open|    high|   low| close|     volume|adj_high| adj_low|adj_close|adj_open| adj_volume|split_factor|dividend|symbol|exchange|               date|        _dlt_load_id|       _dlt_id|\n",
      "+-------+--------+------+------+-----------+--------+--------+---------+--------+-----------+------------+--------+------+--------+-------------------+--------------------+--------------+\n",
      "|379.775|  390.23|379.51|388.56|1.9952846E7|  390.23|  379.51|   388.56| 379.775|1.9952846E7|         1.0|     0.0|  MSFT|    XNAS|2025-03-14 00:00:00|1.7421352563842237E9|Yop916yQPrfOeA|\n",
      "| 211.25|  213.95|209.58|213.49|6.0107582E7|  213.95|  209.58|   213.49|  211.25|6.0107582E7|         1.0|     0.0|  AAPL|    XNAS|2025-03-14 00:00:00|1.7421352563842237E9|FHdPOLiabPdEBQ|\n",
      "| 215.94|216.8394|208.42|209.68|6.0306872E7|216.8394|  208.42|   209.68|  215.95| 6.136833E7|         1.0|     0.0|  AAPL|    XNAS|2025-03-13 00:00:00|1.7421352563842237E9|qErWPnOMnZ0CJA|\n",
      "|383.155|385.3099|377.45|378.77| 2.028023E7|  385.32|  377.45|   378.77| 383.155|2.0473017E7|         1.0|     0.0|  MSFT|    XNAS|2025-03-13 00:00:00|1.7421352563842237E9|V9S6+UaK7EI6dA|\n",
      "| 382.95|  385.22|378.95|383.27|  2.42139E7|385.2165|378.9507|   383.27|  382.95|2.4253567E7|         1.0|     0.0|  MSFT|    XNAS|2025-03-12 00:00:00|1.7421352563842237E9|aE8R/MU7DQDeQw|\n",
      "+-------+--------+------+------+-----------+--------+--------+---------+--------+-----------+------------+--------+------+--------+-------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f56a885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:46:29 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/03/18 12:46:29 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/03/18 12:46:29 INFO FileSourceStrategy: Output Data Schema: struct<open: double, high: double, low: double, close: double, volume: double ... 12 more fields>\n",
      "25/03/18 12:46:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 34.6 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:46:29 INFO BlockManagerInfo: Removed broadcast_3_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 34.6 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:46:29 INFO BlockManagerInfo: Removed broadcast_2_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 8.8 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:46:30 INFO CodeGenerator: Code generated in 223.499614 ms\n",
      "25/03/18 12:46:30 INFO BlockManagerInfo: Removed broadcast_4_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 9.3 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:46:30 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 201.4 KiB, free 434.2 MiB)\n",
      "25/03/18 12:46:30 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.1 MiB)\n",
      "25/03/18 12:46:30 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 34.6 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:46:30 INFO SparkContext: Created broadcast 5 from showString at NativeMethodAccessorImpl.java:0\n",
      "25/03/18 12:46:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/03/18 12:46:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 8.0 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:46:30 INFO DAGScheduler: Registering RDD 18 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "25/03/18 12:46:30 INFO DAGScheduler: Got map stage job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/03/18 12:46:30 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/03/18 12:46:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/03/18 12:46:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 12:46:30 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/03/18 12:46:30 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 83.8 KiB, free 434.1 MiB)\n",
      "25/03/18 12:46:30 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 30.5 KiB, free 434.1 MiB)\n",
      "25/03/18 12:46:30 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 30.5 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:46:30 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 12:46:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 12:46:30 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "25/03/18 12:46:30 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, PROCESS_LOCAL, 4961 bytes) taskResourceAssignments Map()\n",
      "25/03/18 12:46:30 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "25/03/18 12:46:30 INFO CodeGenerator: Code generated in 32.540176 ms\n",
      "25/03/18 12:46:30 INFO CodeGenerator: Code generated in 38.314925 ms\n",
      "25/03/18 12:46:30 INFO CodeGenerator: Code generated in 32.307693 ms\n",
      "25/03/18 12:46:30 INFO CodeGenerator: Code generated in 14.734024 ms\n",
      "25/03/18 12:46:30 INFO FileScanRDD: Reading File path: gs://de-zoomcamp-project-453801-terra-bucket/stock_dataset/stock/1742135256.3842237.3b60b69d81.csv.gz, range: 0-23526, partition values: [empty row]\n",
      "25/03/18 12:46:30 INFO CodeGenerator: Code generated in 28.74038 ms\n",
      "25/03/18 12:46:30 INFO CodecPool: Got brand-new decompressor [.gz]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:46:31 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2833 bytes result sent to driver\n",
      "25/03/18 12:46:31 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 840 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 12:46:31 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/03/18 12:46:31 INFO DAGScheduler: ShuffleMapStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.915 s\n",
      "25/03/18 12:46:31 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/03/18 12:46:31 INFO DAGScheduler: running: Set()\n",
      "25/03/18 12:46:31 INFO DAGScheduler: waiting: Set()\n",
      "25/03/18 12:46:31 INFO DAGScheduler: failed: Set()\n",
      "25/03/18 12:46:31 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/03/18 12:46:31 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/03/18 12:46:31 INFO CodeGenerator: Code generated in 124.751916 ms\n",
      "25/03/18 12:46:31 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/03/18 12:46:31 INFO DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/03/18 12:46:31 INFO DAGScheduler: Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/03/18 12:46:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "25/03/18 12:46:31 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 12:46:31 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[21] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/03/18 12:46:31 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 94.2 KiB, free 434.0 MiB)\n",
      "25/03/18 12:46:31 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 32.4 KiB, free 433.9 MiB)\n",
      "25/03/18 12:46:31 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 32.4 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:46:31 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 12:46:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 12:46:31 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "25/03/18 12:46:31 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/03/18 12:46:31 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)\n",
      "25/03/18 12:46:31 INFO ShuffleBlockFetcherIterator: Getting 1 (4.7 KiB) non-empty blocks including 1 (4.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/03/18 12:46:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms\n",
      "25/03/18 12:46:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 4771 bytes result sent to driver\n",
      "25/03/18 12:46:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 161 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 12:46:31 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "25/03/18 12:46:31 INFO DAGScheduler: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 0.207 s\n",
      "25/03/18 12:46:31 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/03/18 12:46:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "25/03/18 12:46:31 INFO DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.227447 s\n",
      "25/03/18 12:46:31 INFO CodeGenerator: Code generated in 45.096578 ms\n",
      "+------+----------+------------------+--------+-------+------------------+----------+------------------+-----------+------------------+------------------+--------------------+----------------+--------------------+\n",
      "|symbol|year_month|          avg_open|max_high|min_low|         avg_close|max_volume|      avg_adj_high|min_adj_low|     avg_adj_close|      avg_adj_open|      avg_adj_volume|avg_split_factor|        avg_dividend|\n",
      "+------+----------+------------------+--------+-------+------------------+----------+------------------+-----------+------------------+------------------+--------------------+----------------+--------------------+\n",
      "|  MSFT|   2024-06| 436.7971052631579|  456.16| 408.92| 438.3421052631579| 3.41871E7| 440.3619789473684|   408.9234| 438.3421052631579| 436.7821052631579| 1.801949505263158E7|             1.0|                 0.0|\n",
      "|  MSFT|   2024-12|438.82166666666683|456.1648| 420.66|439.40857142857135| 6.42352E7|443.16800952380953|     420.66|439.40857142857135|438.80023809523817|2.0948011666666668E7|             1.0|                 0.0|\n",
      "|  AAPL|   2024-06|206.01236842105263|   220.2| 192.15|206.26315789473685|2.418051E8|209.03628947368418|     192.15|206.26315789473685|205.99421052631578| 9.073602210526316E7|             1.0|                 0.0|\n",
      "|  AAPL|   2025-02|237.32568421052628|  249.98| 225.71| 238.5136842105263|7.007818E7|240.58657368421055|      225.7|238.50052631578944|237.37368421052628|  4.53851062631579E7|             1.0|0.013157894736842105|\n",
      "|  AAPL|   2024-04|169.65188636363638|  178.36| 164.08|169.60454545454547|1.015933E8|171.29227272727272|    164.075|169.60454545454547|169.66090909090914| 5.638230322727273E7|             1.0|                 0.0|\n",
      "+------+----------+------------------+--------+-------+------------------+----------+------------------+-----------+------------------+------------------+--------------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# Assuming you have the DataFrame loaded as `df`\n",
    "\n",
    "# Convert the 'date' column to a proper Date type (if it's not already)\n",
    "df = df.withColumn(\"date\", F.to_date(df[\"date\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "# Extract year and month from the date to perform monthly aggregation\n",
    "df = df.withColumn(\"year_month\", F.date_format(\"date\", \"yyyy-MM\"))\n",
    "\n",
    "# Group by 'symbol' and 'year_month' and calculate the required aggregates\n",
    "monthly_df = df.groupBy(\"symbol\", \"year_month\") \\\n",
    "    .agg(\n",
    "        F.avg(\"open\").alias(\"avg_open\"),\n",
    "        F.max(\"high\").alias(\"max_high\"),\n",
    "        F.min(\"low\").alias(\"min_low\"),\n",
    "        F.avg(\"close\").alias(\"avg_close\"),\n",
    "        F.max(\"volume\").alias(\"max_volume\"),\n",
    "        F.avg(\"adj_high\").alias(\"avg_adj_high\"),\n",
    "        F.min(\"adj_low\").alias(\"min_adj_low\"),\n",
    "        F.avg(\"adj_close\").alias(\"avg_adj_close\"),\n",
    "        F.avg(\"adj_open\").alias(\"avg_adj_open\"),\n",
    "        F.avg(\"adj_volume\").alias(\"avg_adj_volume\"),\n",
    "        F.avg(\"split_factor\").alias(\"avg_split_factor\"),\n",
    "        F.avg(\"dividend\").alias(\"avg_dividend\")\n",
    "    )\n",
    "\n",
    "# Show the result\n",
    "monthly_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c37a45-e513-41bc-a7aa-b023061d2213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:46:41 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/03/18 12:46:41 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/03/18 12:46:41 INFO FileSourceStrategy: Output Data Schema: struct<open: double, high: double, low: double, close: double, volume: double ... 12 more fields>\n",
      "25/03/18 12:46:41 INFO BlockManagerInfo: Removed broadcast_7_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 32.4 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:46:41 INFO BlockManagerInfo: Removed broadcast_5_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 34.6 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:46:41 INFO BlockManagerInfo: Removed broadcast_6_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 30.5 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:46:41 INFO CodeGenerator: Code generated in 80.20447 ms\n",
      "25/03/18 12:46:41 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 201.4 KiB, free 434.2 MiB)\n",
      "25/03/18 12:46:41 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)\n",
      "25/03/18 12:46:41 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 34.6 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:46:41 INFO SparkContext: Created broadcast 8 from showString at NativeMethodAccessorImpl.java:0\n",
      "25/03/18 12:46:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/03/18 12:46:41 INFO DAGScheduler: Registering RDD 25 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "25/03/18 12:46:41 INFO DAGScheduler: Got map stage job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/03/18 12:46:41 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/03/18 12:46:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/03/18 12:46:41 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 12:46:41 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[25] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/03/18 12:46:41 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 84.4 KiB, free 434.1 MiB)\n",
      "25/03/18 12:46:41 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 30.8 KiB, free 434.1 MiB)\n",
      "25/03/18 12:46:41 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 30.8 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:46:41 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 12:46:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[25] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 12:46:41 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "25/03/18 12:46:42 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, PROCESS_LOCAL, 4961 bytes) taskResourceAssignments Map()\n",
      "25/03/18 12:46:42 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)\n",
      "25/03/18 12:46:42 INFO CodeGenerator: Code generated in 11.876648 ms\n",
      "25/03/18 12:46:42 INFO FileScanRDD: Reading File path: gs://de-zoomcamp-project-453801-terra-bucket/stock_dataset/stock/1742135256.3842237.3b60b69d81.csv.gz, range: 0-23526, partition values: [empty row]\n",
      "25/03/18 12:46:42 INFO CodecPool: Got brand-new decompressor [.gz]\n",
      "25/03/18 12:46:42 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 2790 bytes result sent to driver\n",
      "25/03/18 12:46:42 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 295 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 12:46:42 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "25/03/18 12:46:42 INFO DAGScheduler: ShuffleMapStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 0.309 s\n",
      "25/03/18 12:46:42 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/03/18 12:46:42 INFO DAGScheduler: running: Set()\n",
      "25/03/18 12:46:42 INFO DAGScheduler: waiting: Set()\n",
      "25/03/18 12:46:42 INFO DAGScheduler: failed: Set()\n",
      "25/03/18 12:46:42 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/03/18 12:46:42 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/03/18 12:46:42 INFO CodeGenerator: Code generated in 46.909853 ms\n",
      "25/03/18 12:46:42 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/03/18 12:46:42 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/03/18 12:46:42 INFO DAGScheduler: Final stage: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/03/18 12:46:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "25/03/18 12:46:42 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 12:46:42 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[28] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/03/18 12:46:42 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 95.5 KiB, free 434.0 MiB)\n",
      "25/03/18 12:46:42 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 32.8 KiB, free 433.9 MiB)\n",
      "25/03/18 12:46:42 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 32.8 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:46:42 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 12:46:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[28] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 12:46:42 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "25/03/18 12:46:42 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/03/18 12:46:42 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)\n",
      "25/03/18 12:46:42 INFO ShuffleBlockFetcherIterator: Getting 1 (17.3 KiB) non-empty blocks including 1 (17.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/03/18 12:46:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/03/18 12:46:42 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 4778 bytes result sent to driver\n",
      "25/03/18 12:46:42 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 42 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 12:46:42 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "25/03/18 12:46:42 INFO DAGScheduler: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0) finished in 0.067 s\n",
      "25/03/18 12:46:42 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/03/18 12:46:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "25/03/18 12:46:42 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.079290 s\n",
      "25/03/18 12:46:42 INFO CodeGenerator: Code generated in 11.666497 ms\n",
      "+------+---------+------------------+--------+-------+------------------+------------+------------------+-----------+------------------+------------------+--------------+----------------+------------+\n",
      "|symbol|year_week|          avg_open|max_high|min_low|         avg_close|  max_volume|      avg_adj_high|min_adj_low|     avg_adj_close|      avg_adj_open|avg_adj_volume|avg_split_factor|avg_dividend|\n",
      "+------+---------+------------------+--------+-------+------------------+------------+------------------+-----------+------------------+------------------+--------------+----------------+------------+\n",
      "|  AAPL|  2024-18|          175.1144|   187.0| 169.11|173.90400000000002|1.63224109E8|           176.828|     169.11|173.90400000000002|175.08700000000002|  8.83852732E7|             1.0|         0.0|\n",
      "|  AAPL|  2024-21|           190.502|192.8231|186.625|            190.23| 5.0481918E7|          191.8106|    186.625|            190.23|           190.496|  4.17304244E7|             1.0|         0.0|\n",
      "|  AAPL|  2024-33|220.83399999999997|  226.83|  215.6|222.25799999999998|   4.63353E7|223.32139999999998|      215.6|222.25799999999998|220.83399999999997|    4.297965E7|             1.0|        0.05|\n",
      "|  MSFT|  2024-19|410.79799999999994|  415.38| 406.47|           412.096| 1.7782855E7|413.78599999999994|   406.3701|           412.096|411.01900000000006|  1.53798366E7|             1.0|         0.0|\n",
      "|  MSFT|  2024-23| 419.1940000000001|  426.28| 408.92|420.39400000000006|   1.74342E7|421.70799999999997|   408.9234|420.39400000000006|           419.195|  1.54609062E7|             1.0|         0.0|\n",
      "+------+---------+------------------+--------+-------+------------------+------------+------------------+-----------+------------------+------------------+--------------+----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the year and week number from the date\n",
    "df = df.withColumn(\"year\", F.year(df[\"date\"]))\n",
    "df = df.withColumn(\"week\", F.weekofyear(df[\"date\"]))\n",
    "\n",
    "# Combine year and week into a single column 'year_week' in \"yyyy-week\" format\n",
    "df = df.withColumn(\"year_week\", F.concat_ws(\"-\", df[\"year\"], df[\"week\"]))\n",
    "\n",
    "# Group by 'symbol' and 'year_week' and calculate the required aggregates\n",
    "weekly_df = df.groupBy(\"symbol\", \"year_week\") \\\n",
    "    .agg(\n",
    "        F.avg(\"open\").alias(\"avg_open\"),\n",
    "        F.max(\"high\").alias(\"max_high\"),\n",
    "        F.min(\"low\").alias(\"min_low\"),\n",
    "        F.avg(\"close\").alias(\"avg_close\"),\n",
    "        F.max(\"volume\").alias(\"max_volume\"),\n",
    "        F.avg(\"adj_high\").alias(\"avg_adj_high\"),\n",
    "        F.min(\"adj_low\").alias(\"min_adj_low\"),\n",
    "        F.avg(\"adj_close\").alias(\"avg_adj_close\"),\n",
    "        F.avg(\"adj_open\").alias(\"avg_adj_open\"),\n",
    "        F.avg(\"adj_volume\").alias(\"avg_adj_volume\"),\n",
    "        F.avg(\"split_factor\").alias(\"avg_split_factor\"),\n",
    "        F.avg(\"dividend\").alias(\"avg_dividend\")\n",
    "    )\n",
    "\n",
    "# Show the result\n",
    "weekly_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205e4a63-67d6-4050-a3bb-2f827d7f6eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 13:06:37 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/03/18 13:06:37 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/03/18 13:06:37 INFO FileSourceStrategy: Output Data Schema: struct<open: double, high: double, low: double, close: double, volume: double ... 12 more fields>\n",
      "25/03/18 13:06:37 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 13:06:37 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 201.4 KiB, free 434.0 MiB)\n",
      "25/03/18 13:06:37 INFO BlockManagerInfo: Removed broadcast_19_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 34.6 KiB, free: 434.4 MiB)\n",
      "25/03/18 13:06:37 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.2 MiB)\n",
      "25/03/18 13:06:37 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 34.6 KiB, free: 434.4 MiB)\n",
      "25/03/18 13:06:37 INFO SparkContext: Created broadcast 22 from save at BigQueryWriteHelper.java:105\n",
      "25/03/18 13:06:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Registering RDD 53 (save at BigQueryWriteHelper.java:105) as input to shuffle 5\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Got map stage job 14 (save at BigQueryWriteHelper.java:105) with 1 output partitions\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105)\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[53] at save at BigQueryWriteHelper.java:105), which has no missing parents\n",
      "25/03/18 13:06:37 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 85.9 KiB, free 434.1 MiB)\n",
      "25/03/18 13:06:37 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 31.0 KiB, free 434.1 MiB)\n",
      "25/03/18 13:06:37 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 31.0 KiB, free: 434.3 MiB)\n",
      "25/03/18 13:06:37 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[53] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 13:06:37 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "25/03/18 13:06:37 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 14) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, PROCESS_LOCAL, 4961 bytes) taskResourceAssignments Map()\n",
      "25/03/18 13:06:37 INFO Executor: Running task 0.0 in stage 19.0 (TID 14)\n",
      "25/03/18 13:06:37 INFO FileScanRDD: Reading File path: gs://de-zoomcamp-project-453801-terra-bucket/stock_dataset/stock/1742135256.3842237.3b60b69d81.csv.gz, range: 0-23526, partition values: [empty row]\n",
      "25/03/18 13:06:37 INFO CodecPool: Got brand-new decompressor [.gz]\n",
      "25/03/18 13:06:37 INFO Executor: Finished task 0.0 in stage 19.0 (TID 14). 2790 bytes result sent to driver\n",
      "25/03/18 13:06:37 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 14) in 176 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 13:06:37 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "25/03/18 13:06:37 INFO DAGScheduler: ShuffleMapStage 19 (save at BigQueryWriteHelper.java:105) finished in 0.189 s\n",
      "25/03/18 13:06:37 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/03/18 13:06:37 INFO DAGScheduler: running: Set()\n",
      "25/03/18 13:06:37 INFO DAGScheduler: waiting: Set()\n",
      "25/03/18 13:06:37 INFO DAGScheduler: failed: Set()\n",
      "25/03/18 13:06:37 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/03/18 13:06:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 13:06:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 13:06:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 13:06:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 13:06:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 13:06:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 13:06:37 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/03/18 13:06:37 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Got job 15 (save at BigQueryWriteHelper.java:105) with 1 output partitions\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Final stage: ResultStage 21 (save at BigQueryWriteHelper.java:105)\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[55] at save at BigQueryWriteHelper.java:105), which has no missing parents\n",
      "25/03/18 13:06:37 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 294.1 KiB, free 433.8 MiB)\n",
      "25/03/18 13:06:37 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 103.3 KiB, free 433.7 MiB)\n",
      "25/03/18 13:06:37 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 103.3 KiB, free: 434.2 MiB)\n",
      "25/03/18 13:06:37 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 13:06:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[55] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 13:06:37 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
      "25/03/18 13:06:37 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 15) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/03/18 13:06:37 INFO Executor: Running task 0.0 in stage 21.0 (TID 15)\n",
      "25/03/18 13:06:37 INFO ShuffleBlockFetcherIterator: Getting 1 (4.7 KiB) non-empty blocks including 1 (4.7 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/03/18 13:06:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "25/03/18 13:06:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 13:06:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 13:06:37 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 13:06:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 13:06:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 13:06:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 13:06:37 INFO CodecConfig: Compression: SNAPPY\n",
      "25/03/18 13:06:37 INFO CodecConfig: Compression: SNAPPY\n",
      "25/03/18 13:06:37 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/03/18 13:06:37 INFO ParquetOutputFormat: Validation is off\n",
      "25/03/18 13:06:37 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/03/18 13:06:37 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/03/18 13:06:37 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"symbol\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"year_month\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_open\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_high\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_low\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_close\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_volume\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_adj_high\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_adj_low\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_adj_close\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_adj_open\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_adj_volume\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_split_factor\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_dividend\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary symbol (STRING);\n",
      "  optional binary year_month (STRING);\n",
      "  optional double avg_open;\n",
      "  optional double max_high;\n",
      "  optional double min_low;\n",
      "  optional double avg_close;\n",
      "  optional double max_volume;\n",
      "  optional double avg_adj_high;\n",
      "  optional double min_adj_low;\n",
      "  optional double avg_adj_close;\n",
      "  optional double avg_adj_open;\n",
      "  optional double avg_adj_volume;\n",
      "  optional double avg_split_factor;\n",
      "  optional double avg_dividend;\n",
      "}\n",
      "\n",
      "       \n",
      "25/03/18 13:06:38 INFO BlockManagerInfo: Removed broadcast_23_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 31.0 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 13:06:38 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-b7ef6750-8029-4e8d-ad36-016bee1a9f92/_temporary/0/_temporary/' directory.\n",
      "25/03/18 13:06:38 INFO FileOutputCommitter: Saved output of task 'attempt_202503181306378933985505372282929_0021_m_000000_15' to gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-b7ef6750-8029-4e8d-ad36-016bee1a9f92/_temporary/0/task_202503181306378933985505372282929_0021_m_000000\n",
      "25/03/18 13:06:38 INFO SparkHadoopMapRedUtil: attempt_202503181306378933985505372282929_0021_m_000000_15: Committed. Elapsed time: 443 ms.\n",
      "25/03/18 13:06:38 INFO Executor: Finished task 0.0 in stage 21.0 (TID 15). 4854 bytes result sent to driver\n",
      "25/03/18 13:06:38 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 15) in 821 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 13:06:38 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "25/03/18 13:06:38 INFO DAGScheduler: ResultStage 21 (save at BigQueryWriteHelper.java:105) finished in 0.862 s\n",
      "25/03/18 13:06:38 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/03/18 13:06:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "25/03/18 13:06:38 INFO DAGScheduler: Job 15 finished: save at BigQueryWriteHelper.java:105, took 0.868736 s\n",
      "25/03/18 13:06:38 INFO FileFormatWriter: Start to commit write Job 4e2c13fb-f65a-4edb-8c79-dcfbabd0a99b.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 13:06:39 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-b7ef6750-8029-4e8d-ad36-016bee1a9f92/_temporary/0/task_202503181306378933985505372282929_0021_m_000000/' directory.\n",
      "25/03/18 13:06:39 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-b7ef6750-8029-4e8d-ad36-016bee1a9f92/' directory.\n",
      "25/03/18 13:06:39 INFO BlockManagerInfo: Removed broadcast_24_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 103.3 KiB, free: 434.4 MiB)\n",
      "25/03/18 13:06:39 INFO FileFormatWriter: Write Job 4e2c13fb-f65a-4edb-8c79-dcfbabd0a99b committed. Elapsed time: 975 ms.\n",
      "25/03/18 13:06:39 INFO FileFormatWriter: Finished processing stats for write job 4e2c13fb-f65a-4edb-8c79-dcfbabd0a99b.\n",
      "25/03/18 13:06:40 ERROR BigQueryClient: Unable to create the job to load to de-zoomcamp-project-453801.demo_dataset.monthly_stock_data\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o179.save.\n: com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:110)\n\tat com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: The field specified for time partitioning can only be of type TIMESTAMP, DATE or DATETIME. The type found is: STRING.\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:220)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:405)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:394)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryRetryHelper.run(BigQueryRetryHelper.java:86)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryRetryHelper.runWithRetries(BigQueryRetryHelper.java:49)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:393)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:358)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:328)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:323)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.loadDataIntoTable(BigQueryClient.java:564)\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.loadDataToBigQuery(BigQueryWriteHelper.java:134)\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:107)\n\t... 44 more\nCaused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\nPOST https://www.googleapis.com/bigquery/v2/projects/de-zoomcamp-project-453801/jobs?prettyPrint=false\n{\n  \"code\": 400,\n  \"errors\": [\n    {\n      \"domain\": \"global\",\n      \"message\": \"The field specified for time partitioning can only be of type TIMESTAMP, DATE or DATETIME. The type found is: STRING.\",\n      \"reason\": \"invalid\"\n    }\n  ],\n  \"message\": \"The field specified for time partitioning can only be of type TIMESTAMP, DATE or DATETIME. The type found is: STRING.\",\n  \"status\": \"INVALID_ARGUMENT\"\n}\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$3.interceptResponse(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:552)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:493)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:603)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:218)\n\t... 56 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m monthly_df\u001b[38;5;241m.\u001b[39mwrite \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigquery\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemporaryGcsBucket\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde-zoomcamp-project-453801-terra-bucket\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde-zoomcamp-project-453801.demo_dataset.monthly_stock_data\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartitionField\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/readwriter.py:966\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o179.save.\n: com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:110)\n\tat com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: The field specified for time partitioning can only be of type TIMESTAMP, DATE or DATETIME. The type found is: STRING.\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:220)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:405)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:394)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryRetryHelper.run(BigQueryRetryHelper.java:86)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryRetryHelper.runWithRetries(BigQueryRetryHelper.java:49)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:393)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:358)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:328)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:323)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.loadDataIntoTable(BigQueryClient.java:564)\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.loadDataToBigQuery(BigQueryWriteHelper.java:134)\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:107)\n\t... 44 more\nCaused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\nPOST https://www.googleapis.com/bigquery/v2/projects/de-zoomcamp-project-453801/jobs?prettyPrint=false\n{\n  \"code\": 400,\n  \"errors\": [\n    {\n      \"domain\": \"global\",\n      \"message\": \"The field specified for time partitioning can only be of type TIMESTAMP, DATE or DATETIME. The type found is: STRING.\",\n      \"reason\": \"invalid\"\n    }\n  ],\n  \"message\": \"The field specified for time partitioning can only be of type TIMESTAMP, DATE or DATETIME. The type found is: STRING.\",\n  \"status\": \"INVALID_ARGUMENT\"\n}\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$3.interceptResponse(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:552)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:493)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:603)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:218)\n\t... 56 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 13:14:04 INFO BlockManagerInfo: Removed broadcast_22_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 34.6 KiB, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "#  #.option(\"clusteredFields\", \"symbol, year_month\") \\\n",
    "monthly_df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"temporaryGcsBucket\", \"de-zoomcamp-project-453801-terra-bucket\") \\\n",
    "    .option(\"table\", \"de-zoomcamp-project-453801.demo_dataset.monthly_stock_data\") \\\n",
    "    .option(\"partitionField\", \"symbol\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3001afa-1889-4280-8c0b-748210a5418f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:49:30 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/03/18 12:49:30 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/03/18 12:49:30 INFO FileSourceStrategy: Output Data Schema: struct<open: double, high: double, low: double, close: double, volume: double ... 12 more fields>\n",
      "25/03/18 12:49:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 12:49:30 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 201.4 KiB, free 434.0 MiB)\n",
      "25/03/18 12:49:30 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 433.9 MiB)\n",
      "25/03/18 12:49:30 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 34.6 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:49:30 INFO SparkContext: Created broadcast 14 from save at BigQueryWriteHelper.java:105\n",
      "25/03/18 12:49:30 INFO BlockManagerInfo: Removed broadcast_11_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 34.6 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:49:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Registering RDD 38 (save at BigQueryWriteHelper.java:105) as input to shuffle 3\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Got map stage job 9 (save at BigQueryWriteHelper.java:105) with 1 output partitions\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105)\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[38] at save at BigQueryWriteHelper.java:105), which has no missing parents\n",
      "25/03/18 12:49:30 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 86.6 KiB, free 434.1 MiB)\n",
      "25/03/18 12:49:30 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.2 KiB, free 434.1 MiB)\n",
      "25/03/18 12:49:30 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 31.2 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:49:30 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[38] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 12:49:30 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "25/03/18 12:49:30 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, PROCESS_LOCAL, 4961 bytes) taskResourceAssignments Map()\n",
      "25/03/18 12:49:30 INFO Executor: Running task 0.0 in stage 12.0 (TID 9)\n",
      "25/03/18 12:49:30 INFO FileScanRDD: Reading File path: gs://de-zoomcamp-project-453801-terra-bucket/stock_dataset/stock/1742135256.3842237.3b60b69d81.csv.gz, range: 0-23526, partition values: [empty row]\n",
      "25/03/18 12:49:30 INFO CodecPool: Got brand-new decompressor [.gz]\n",
      "25/03/18 12:49:30 INFO Executor: Finished task 0.0 in stage 12.0 (TID 9). 2790 bytes result sent to driver\n",
      "25/03/18 12:49:30 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 237 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 12:49:30 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "25/03/18 12:49:30 INFO DAGScheduler: ShuffleMapStage 12 (save at BigQueryWriteHelper.java:105) finished in 0.266 s\n",
      "25/03/18 12:49:30 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/03/18 12:49:30 INFO DAGScheduler: running: Set()\n",
      "25/03/18 12:49:30 INFO DAGScheduler: waiting: Set()\n",
      "25/03/18 12:49:30 INFO DAGScheduler: failed: Set()\n",
      "25/03/18 12:49:30 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/03/18 12:49:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 12:49:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 12:49:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 12:49:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 12:49:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 12:49:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 12:49:30 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/03/18 12:49:30 INFO CodeGenerator: Code generated in 28.689886 ms\n",
      "25/03/18 12:49:30 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Got job 10 (save at BigQueryWriteHelper.java:105) with 1 output partitions\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Final stage: ResultStage 14 (save at BigQueryWriteHelper.java:105)\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[40] at save at BigQueryWriteHelper.java:105), which has no missing parents\n",
      "25/03/18 12:49:30 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 295.5 KiB, free 433.8 MiB)\n",
      "25/03/18 12:49:30 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 103.6 KiB, free 433.7 MiB)\n",
      "25/03/18 12:49:30 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 103.6 KiB, free: 434.2 MiB)\n",
      "25/03/18 12:49:30 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 12:49:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[40] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 12:49:30 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "25/03/18 12:49:30 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 10) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "25/03/18 12:49:30 INFO Executor: Running task 0.0 in stage 14.0 (TID 10)\n",
      "25/03/18 12:49:30 INFO ShuffleBlockFetcherIterator: Getting 1 (17.3 KiB) non-empty blocks including 1 (17.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/03/18 12:49:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/03/18 12:49:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 12:49:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 12:49:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 12:49:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 12:49:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 12:49:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 12:49:30 INFO CodecConfig: Compression: SNAPPY\n",
      "25/03/18 12:49:30 INFO CodecConfig: Compression: SNAPPY\n",
      "25/03/18 12:49:30 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/03/18 12:49:30 INFO ParquetOutputFormat: Validation is off\n",
      "25/03/18 12:49:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/03/18 12:49:30 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/03/18 12:49:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"symbol\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"year_week\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_open\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_high\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_low\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_close\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"max_volume\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_adj_high\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"min_adj_low\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_adj_close\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_adj_open\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_adj_volume\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_split_factor\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"avg_dividend\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary symbol (STRING);\n",
      "  required binary year_week (STRING);\n",
      "  optional double avg_open;\n",
      "  optional double max_high;\n",
      "  optional double min_low;\n",
      "  optional double avg_close;\n",
      "  optional double max_volume;\n",
      "  optional double avg_adj_high;\n",
      "  optional double min_adj_low;\n",
      "  optional double avg_adj_close;\n",
      "  optional double avg_adj_open;\n",
      "  optional double avg_adj_volume;\n",
      "  optional double avg_split_factor;\n",
      "  optional double avg_dividend;\n",
      "}\n",
      "\n",
      "       \n",
      "25/03/18 12:49:31 INFO BlockManagerInfo: Removed broadcast_15_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 31.2 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:49:31 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-957dce8e-03c5-4561-a07c-f9ad8e86e139/_temporary/0/_temporary/' directory.\n",
      "25/03/18 12:49:31 INFO FileOutputCommitter: Saved output of task 'attempt_202503181249302461627306395990593_0014_m_000000_10' to gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-957dce8e-03c5-4561-a07c-f9ad8e86e139/_temporary/0/task_202503181249302461627306395990593_0014_m_000000\n",
      "25/03/18 12:49:31 INFO SparkHadoopMapRedUtil: attempt_202503181249302461627306395990593_0014_m_000000_10: Committed. Elapsed time: 443 ms.\n",
      "25/03/18 12:49:31 INFO Executor: Finished task 0.0 in stage 14.0 (TID 10). 4854 bytes result sent to driver\n",
      "25/03/18 12:49:31 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 10) in 848 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 12:49:31 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "25/03/18 12:49:31 INFO DAGScheduler: ResultStage 14 (save at BigQueryWriteHelper.java:105) finished in 0.875 s\n",
      "25/03/18 12:49:31 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/03/18 12:49:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "25/03/18 12:49:31 INFO DAGScheduler: Job 10 finished: save at BigQueryWriteHelper.java:105, took 0.881274 s\n",
      "25/03/18 12:49:31 INFO FileFormatWriter: Start to commit write Job 9793a380-f214-43e8-8ae7-c0be446b7e6f.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:49:32 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-957dce8e-03c5-4561-a07c-f9ad8e86e139/_temporary/0/task_202503181249302461627306395990593_0014_m_000000/' directory.\n",
      "25/03/18 12:49:32 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-957dce8e-03c5-4561-a07c-f9ad8e86e139/' directory.\n",
      "25/03/18 12:49:32 INFO BlockManagerInfo: Removed broadcast_16_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 103.6 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:49:32 INFO FileFormatWriter: Write Job 9793a380-f214-43e8-8ae7-c0be446b7e6f committed. Elapsed time: 1016 ms.\n",
      "25/03/18 12:49:32 INFO FileFormatWriter: Finished processing stats for write job 9793a380-f214-43e8-8ae7-c0be446b7e6f.\n",
      "25/03/18 12:49:33 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=demo_dataset, projectId=de-zoomcamp-project-453801, tableId=weekly_stock_data}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=symbol, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year_week, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=avg_open, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_high, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_low, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=avg_close, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=max_volume, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=avg_adj_high, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=min_adj_low, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=avg_adj_close, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=avg_adj_open, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=avg_adj_volume, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=avg_split_factor, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=avg_dividend, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-957dce8e-03c5-4561-a07c-f9ad8e86e139/part-00000-db0c1f64-d461-4a6f-862a-d8a24f260299-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=de-zoomcamp-project-453801, job=eaab23d1-e529-4bfb-8ca9-7146ed83480c, location=EU}\n",
      "25/03/18 12:49:35 INFO BigQueryClient: Done loading to de-zoomcamp-project-453801.demo_dataset.weekly_stock_data. jobId: JobId{project=de-zoomcamp-project-453801, job=eaab23d1-e529-4bfb-8ca9-7146ed83480c, location=EU}\n"
     ]
    }
   ],
   "source": [
    "weekly_df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"temporaryGcsBucket\", \"de-zoomcamp-project-453801-terra-bucket\") \\\n",
    "    .option(\"table\", \"de-zoomcamp-project-453801.demo_dataset.weekly_stock_data\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d2d3ac1-cd9e-4a2c-8b94-ac489fe8eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:49:53 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/03/18 12:49:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/03/18 12:49:53 INFO FileSourceStrategy: Output Data Schema: struct<open: double, high: double, low: double, close: double, volume: double ... 15 more fields>\n",
      "25/03/18 12:49:53 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 12:49:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 12:49:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 12:49:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 12:49:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 12:49:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 12:49:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 12:49:53 INFO CodeGenerator: Code generated in 20.890983 ms\n",
      "25/03/18 12:49:53 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 201.4 KiB, free 434.0 MiB)\n",
      "25/03/18 12:49:53 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 434.1 MiB)\n",
      "25/03/18 12:49:53 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 34.6 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:49:53 INFO SparkContext: Created broadcast 17 from save at BigQueryWriteHelper.java:105\n",
      "25/03/18 12:49:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/03/18 12:49:53 INFO BlockManagerInfo: Removed broadcast_14_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 34.6 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:49:53 INFO SparkContext: Starting job: save at BigQueryWriteHelper.java:105\n",
      "25/03/18 12:49:53 INFO DAGScheduler: Got job 11 (save at BigQueryWriteHelper.java:105) with 1 output partitions\n",
      "25/03/18 12:49:53 INFO DAGScheduler: Final stage: ResultStage 15 (save at BigQueryWriteHelper.java:105)\n",
      "25/03/18 12:49:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/03/18 12:49:53 INFO DAGScheduler: Missing parents: List()\n",
      "25/03/18 12:49:53 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[43] at save at BigQueryWriteHelper.java:105), which has no missing parents\n",
      "25/03/18 12:49:53 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 228.5 KiB, free 433.9 MiB)\n",
      "25/03/18 12:49:53 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 81.8 KiB, free 433.9 MiB)\n",
      "25/03/18 12:49:53 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 (size: 81.8 KiB, free: 434.3 MiB)\n",
      "25/03/18 12:49:53 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\n",
      "25/03/18 12:49:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[43] at save at BigQueryWriteHelper.java:105) (first 15 tasks are for partitions Vector(0))\n",
      "25/03/18 12:49:53 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "25/03/18 12:49:53 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 11) (de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal, executor driver, partition 0, PROCESS_LOCAL, 4972 bytes) taskResourceAssignments Map()\n",
      "25/03/18 12:49:53 INFO Executor: Running task 0.0 in stage 15.0 (TID 11)\n",
      "25/03/18 12:49:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 12:49:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 12:49:54 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 12:49:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/03/18 12:49:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/03/18 12:49:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "25/03/18 12:49:54 INFO CodecConfig: Compression: SNAPPY\n",
      "25/03/18 12:49:54 INFO CodecConfig: Compression: SNAPPY\n",
      "25/03/18 12:49:54 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "25/03/18 12:49:54 INFO ParquetOutputFormat: Validation is off\n",
      "25/03/18 12:49:54 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "25/03/18 12:49:54 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "25/03/18 12:49:54 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"open\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"high\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"low\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"close\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"volume\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"adj_high\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"adj_low\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"adj_close\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"adj_open\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"adj_volume\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"split_factor\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"dividend\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"symbol\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"exchange\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"date\",\n",
      "    \"type\" : \"date\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"_dlt_load_id\",\n",
      "    \"type\" : \"double\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"_dlt_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"year_month\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"year\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"week\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"year_week\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : false,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional double open;\n",
      "  optional double high;\n",
      "  optional double low;\n",
      "  optional double close;\n",
      "  optional double volume;\n",
      "  optional double adj_high;\n",
      "  optional double adj_low;\n",
      "  optional double adj_close;\n",
      "  optional double adj_open;\n",
      "  optional double adj_volume;\n",
      "  optional double split_factor;\n",
      "  optional double dividend;\n",
      "  optional binary symbol (STRING);\n",
      "  optional binary exchange (STRING);\n",
      "  optional int32 date (DATE);\n",
      "  optional double _dlt_load_id;\n",
      "  optional binary _dlt_id (STRING);\n",
      "  optional binary year_month (STRING);\n",
      "  optional int32 year;\n",
      "  optional int32 week;\n",
      "  required binary year_week (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "25/03/18 12:49:54 INFO FileScanRDD: Reading File path: gs://de-zoomcamp-project-453801-terra-bucket/stock_dataset/stock/1742135256.3842237.3b60b69d81.csv.gz, range: 0-23526, partition values: [empty row]\n",
      "25/03/18 12:49:54 INFO CodecPool: Got brand-new decompressor [.gz]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:49:55 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-5524d064-a83c-4183-9557-25cd029a757d/_temporary/0/_temporary/' directory.\n",
      "25/03/18 12:49:55 INFO FileOutputCommitter: Saved output of task 'attempt_202503181249537726674266397982598_0015_m_000000_11' to gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-5524d064-a83c-4183-9557-25cd029a757d/_temporary/0/task_202503181249537726674266397982598_0015_m_000000\n",
      "25/03/18 12:49:55 INFO SparkHadoopMapRedUtil: attempt_202503181249537726674266397982598_0015_m_000000_11: Committed. Elapsed time: 454 ms.\n",
      "25/03/18 12:49:55 INFO Executor: Finished task 0.0 in stage 15.0 (TID 11). 2566 bytes result sent to driver\n",
      "25/03/18 12:49:55 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 11) in 1212 ms on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal (executor driver) (1/1)\n",
      "25/03/18 12:49:55 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "25/03/18 12:49:55 INFO DAGScheduler: ResultStage 15 (save at BigQueryWriteHelper.java:105) finished in 1.260 s\n",
      "25/03/18 12:49:55 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/03/18 12:49:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
      "25/03/18 12:49:55 INFO DAGScheduler: Job 11 finished: save at BigQueryWriteHelper.java:105, took 1.265182 s\n",
      "25/03/18 12:49:55 INFO FileFormatWriter: Start to commit write Job bfd07b87-c375-4c9b-aa9b-1e484a6a30e3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/03/18 12:49:55 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-5524d064-a83c-4183-9557-25cd029a757d/_temporary/0/task_202503181249537726674266397982598_0015_m_000000/' directory.\n",
      "25/03/18 12:49:55 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-5524d064-a83c-4183-9557-25cd029a757d/' directory.\n",
      "25/03/18 12:49:56 INFO BlockManagerInfo: Removed broadcast_18_piece0 on de-zoomcamp-project.europe-west1-b.c.de-zoomcamp-project-453801.internal:46511 in memory (size: 81.8 KiB, free: 434.4 MiB)\n",
      "25/03/18 12:49:56 INFO FileFormatWriter: Write Job bfd07b87-c375-4c9b-aa9b-1e484a6a30e3 committed. Elapsed time: 978 ms.\n",
      "25/03/18 12:49:56 INFO FileFormatWriter: Finished processing stats for write job bfd07b87-c375-4c9b-aa9b-1e484a6a30e3.\n",
      "25/03/18 12:49:56 INFO BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=demo_dataset, projectId=de-zoomcamp-project-453801, tableId=daily_stock_data}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=FormatOptions{format=PARQUET}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=open, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=high, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=low, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=close, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=volume, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=adj_high, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=adj_low, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=adj_close, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=adj_open, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=adj_volume, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=split_factor, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dividend, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=symbol, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=exchange, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=date, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=_dlt_load_id, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=_dlt_id, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year_month, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=week, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year_week, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://de-zoomcamp-project-453801-terra-bucket/.spark-bigquery-local-1742301844485-5524d064-a83c-4183-9557-25cd029a757d/part-00000-87e3cf31-e705-4f29-b5f7-f8136fff7db8-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null}. jobId: JobId{project=de-zoomcamp-project-453801, job=38005505-1135-4987-8b29-799886454729, location=EU}\n",
      "25/03/18 12:49:59 INFO BigQueryClient: Done loading to de-zoomcamp-project-453801.demo_dataset.daily_stock_data. jobId: JobId{project=de-zoomcamp-project-453801, job=38005505-1135-4987-8b29-799886454729, location=EU}\n"
     ]
    }
   ],
   "source": [
    "df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"temporaryGcsBucket\", \"de-zoomcamp-project-453801-terra-bucket\") \\\n",
    "    .option(\"table\", \"de-zoomcamp-project-453801.demo_dataset.daily_stock_data\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c17796-3dcc-4336-80ea-93f8e1a8a796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

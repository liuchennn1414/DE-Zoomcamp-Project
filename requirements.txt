dlt 
python 
requests 

-- google cloud auth 
curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-460.0.0-linux-x86_64.tar.gz
tar -xf google-cloud-cli-460.0.0-linux-x86_64.tar.gz
./google-cloud-sdk/install.sh
# test download 
gcloud --version 
# export credentials 
export GOOGLE_APPLICATION_CREDENTIALS="/home/xxx/.gc/my-creds.json"
# authentification 
gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS
# test access 
gsutil ls gs://xxx
# export projectID variable

-- for pipeline config 
export CREDENTIALS__PROJECT_ID="dxxx"
export DESTINATION__FILESYSTEM__CREDENTIALS__CLIENT_EMAIL="xxx"
export DESTINATION__FILESYSTEM__CREDENTIALS__PRIVATE_KEY="/home/chenchen/.ssh/gcp"
export GOOGLE_APPLICATION_CREDENTIALS="/home/chenchen/.gc/my-creds.json"
export API_ACCESS_KEY="xxxxxxxx"
pip install "dlt[gs]"

set up spark 
https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/linux.md
connect to GCP 
wget https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar
cp gcs-connector-hadoop3-latest.jar $SPARK_HOME/jars/

mkdir ~/hadoop-conf
nano ~/hadoop-conf/core-site.xml -- copy the config file in 
export HADOOP_CONF_DIR=~/hadoop-conf
echo 'export HADOOP_CONF_DIR=~/hadoop-conf' >> ~/.bashrc
source ~/.bashrc
